# Predictive Water Intelligence - Barcelona Water Flow

Sistema de manteniment predictiu per detectar possibles comportaments de subcomptatge en comptadors d'aigua intel¬∑ligents a Barcelona. Aquest projecte implementa un pipeline d'aprenentatge no supervisat multi-etapa que identifica comptadors d'alt risc que requereixen inspecci√≥ o manteniment.

## üéØ Visi√≥ General del Projecte

Aquesta aplicaci√≥ ajuda Aig√ºes de Barcelona a monitoritzar i mantenir la seva infraestructura de comptadors d'aigua mitjan√ßant:

- **Identificaci√≥ de comptadors an√≤mals**: Detecci√≥ de comptadors que mostren patrons de consum inusuals
- **Puntuaci√≥ de risc**: Assignaci√≥ d'una probabilitat de fallada (0-100%) a cada comptador
- **Visualitzaci√≥ interactiva**: Interf√≠cie basada en mapa per explorar l'estat de salut dels comptadors
- **Insights accionables**: Generaci√≥ de informes detallats sobre els 20 comptadors amb major risc

---

# üìã Guia Completa d'Execuci√≥ del Projecte

Aquesta guia explica pas a pas com executar tot el codi del projecte des de zero.

## Pas 1: Preparaci√≥ del Dataset

### 1.1. Col¬∑locar el Dataset Original

Col¬∑loca el fitxer del dataset original a la carpeta `data/data/` amb un d'aquests noms:
- `Dades_Comptadors_anonymized_v2.csv` (format CSV)
- `Dades_Comptadors_anonymized_v2.parquet` (format Parquet - **recomanat**)

**Estructura esperada del dataset:**

El dataset ha de contenir les seg√ºents columnes:
- `POLIZA_SUMINISTRO`: Identificador √∫nic del comptador
- `FECHA`: Data del registre (format YYYY-MM-DD)
- `CONSUMO_REAL`: Consum real d'aigua (litres/dia)
- `SECCIO_CENSAL`: Codi de secci√≥ censal
- `US_AIGUA_GEST`: Tipus d'√∫s ('D'=dom√®stic, 'C'=comercial, 'I'=industrial, 'A'=altres)
- `NUM_MUN_SGAB`: Codi de municipi
- `NUM_DTE_MUNI`: Codi de districte
- `NUM_COMPLET`: Identificador complet del comptador
- `DATA_INST_COMP`: Data d'instal¬∑laci√≥ del comptador
- `MARCA_COMP`: Marca del comptador
- `CODI_MODEL`: Codi del model
- `DIAM_COMP`: Di√†metre del comptador (mm)

### 1.2. Convertir CSV a Parquet (si cal)

Si tens el dataset en format CSV, converteix-lo a Parquet per millor rendiment:

```python
import pandas as pd
from pathlib import Path

# Llegeix el CSV
csv_path = Path("data/data/Dades_Comptadors_anonymized_v2.csv")
df = pd.read_csv(csv_path)

# Guarda com a Parquet
parquet_path = Path("data/data/Dades_Comptadors_anonymized_v2.parquet")
df.to_parquet(parquet_path, index=False, engine='pyarrow')

print(f"‚úì Dataset convertit a: {parquet_path}")
```

---

## Pas 2: Instal¬∑laci√≥ de Depend√®ncies

### 2.1. Depend√®ncies Python

```bash
cd data
pip install -r requirements.txt
```

Aix√≤ instal¬∑lar√†:
- pandas, numpy, scipy
- scikit-learn
- torch (PyTorch)
- duckdb
- matplotlib, seaborn
- pyarrow (per llegir/escrivir Parquet)
- joblib (per guardar models)
- shapely (per dades geogr√†fiques)

### 2.2. Depend√®ncies del Frontend

```bash
# Des de l'arrel del projecte
npm install
```

---

## Pas 3: Creaci√≥ de la Base de Dades DuckDB

Abans d'executar les etapes, cal crear la base de dades DuckDB que utilitzaran les etapes:

```bash
cd data
python create_database.py
```

Aquest script:
- Llegeix el fitxer Parquet de `data/data/Dades_Comptadors_anonymized_v2.parquet`
- Crea la base de dades `analytics.duckdb` amb dues vistes:
  - `counter_metadata`: Metadades dels comptadors (caracter√≠stiques f√≠siques)
  - `consumption_data`: Dades de consum diari

**Sortida esperada:**
```
Creating database with views...
Views created:
  - counter_metadata: [n√∫mero] rows
  - consumption_data: [n√∫mero] rows
[OK] Database created: data/analytics.duckdb
```

---

## Pas 4: Execuci√≥ de les Etapes del Pipeline

Executa les etapes en ordre seq√ºencial. Cada etapa genera sortides que s√≥n entrada per l'etapa seg√ºent.

### Etapa 0: An√†lisi Explorat√≤ria (Opcional)

```bash
cd data
# Obre el notebook Jupyter
jupyter notebook eda_full_dataset.ipynb
```

Aquest notebook analitza la qualitat i distribuci√≥ de les dades abans de la modelitzaci√≥.

### Etapa I: Caracter√≠stiques F√≠siques i KMeans

```bash
cd data
python -m stage1_kmeans.run_stage1
```

**Qu√® fa:**
- Extreu caracter√≠stiques f√≠siques (edat, di√†metre, canya, marca/model)
- Normalitza les caracter√≠stiques
- Troba el k √≤ptim mitjan√ßant silueta (prova k de 2 a 20)
- Aplica KMeans per generar pseudo-etiquetes de cluster

**Sortides:**
- `stage1_outputs/stage1_physical_features_with_clusters.csv`
- Model KMeans guardat (si s'ha configurat)

**Temps estimat:** 2-5 minuts

### Etapa II: Entrenament de l'Autoencoder

```bash
cd data
python -m stage2_autoencoder.run_stage2
```

**Qu√® fa:**
- Construeix vectors d'entrada amb 48 valors de consum mensual + caracter√≠stiques f√≠siques + etiqueta de cluster
- Entrena un autoencoder per aprendre representacions latents
- Extreu els vectors latents Z per a tots els comptadors

**Sortides:**
- `stage2_outputs/latent_representations.csv` (matriu [num_comptadors √ó dimensi√≥_latent])
- `models/stage2_autoencoder.pth` (model entrenat)

**Temps estimat:** 10-30 minuts (dep√®n de la GPU)

### Etapa III: Clustering de l'Espai Latent

```bash
cd data
python -m stage3_clustering.run_stage3
```

**Qu√® fa:**
- Aplica KMeans (o DBSCAN) sobre els vectors latents de l'Etapa II
- Genera clusters de perfils comportamentals
- Realitza an√†lisi estad√≠stica per identificar clusters de risc

**Sortides:**
- `stage3_outputs/cluster_labels.csv`
- `stage3_outputs/cluster_analysis_*.csv` (an√†lisis per edat, canya, di√†metre, marca/model)
- `stage3_outputs/cluster_analysis_subcounting_risk.csv` (clusters ordenats per risc)
- `stage3_outputs/visualizations/*.png` (gr√†fics d'an√†lisi)
- `models/stage3_kmeans_clustering.joblib` (model de clustering)

**Temps estimat:** 3-8 minuts

### Etapa IV: C√†lcul de Probabilitats de Risc

```bash
cd data
python -m stage4_risk_probabilities.run_stage4
```

**Qu√® fa:**
- Calcula la puntuaci√≥ d'anomalia intra-cluster (dist√†ncia al centroide)
- Calcula la degradaci√≥ a nivell de cluster (edat + canya)
- Combina aquests components per obtenir el risc base
- Calcula la probabilitat de subcomptatge a partir de les s√®ries temporals
- Combina risc base i subcomptatge per obtenir el risc final

**Sortides:**
- `stage4_outputs/meter_failure_risk.csv` (risc per a cada comptador)
- `stage4_outputs/risk_summary_by_cluster.csv` (estad√≠stiques per cluster)
- `stage4_outputs/visualizations/*.png` (distribucions de risc)

**Temps estimat:** 5-15 minuts

**Par√†metres opcionals:**
```bash
python -m stage4_risk_probabilities.run_stage4 \
    --w1 0.5 \              # Pes per puntuaci√≥ d'anomalia
    --w2 0.5 \              # Pes per degradaci√≥ de cluster
    --alpha 0.6 \           # Pes per edat en degradaci√≥
    --beta 0.4 \            # Pes per canya en degradaci√≥
    --subcount-gamma 0.8 \  # Pes m√†xim per subcomptatge
    --disable-subcounting   # Desactivar c√†lcul de subcomptatge
```

### Pas 5: Preparaci√≥ de Dades per al Mapa

```bash
cd data
python prepare_map_data.py
```

**Qu√® fa:**
- Llegeix els resultats de l'Etapa IV (`stage4_outputs/meter_failure_risk.csv`)
- Fusiona amb metadades geogr√†fiques de la base de dades
- Genera fitxers GeoJSON per al frontend:
  - `public/data/water_meters.geojson` (punts dels comptadors amb risc)
  - `public/data/census_sections.geojson` (seccions censals agregades)
  - `public/data/risk_summary.json` (resum estad√≠stic)

**Sortida esperada:**
```
Loading risk data...
  Loaded [n√∫mero] meters with risk scores
Loading metadata...
  Loaded [n√∫mero] meters with metadata
Preparing meter points...
  Generated [n√∫mero] meter point features
Preparing census sections...
  Generated [n√∫mero] census section features
Saving GeoJSON files...
  ‚úì public/data/water_meters.geojson
  ‚úì public/data/census_sections.geojson
  ‚úì public/data/risk_summary.json
```

**Temps estimat:** 1-3 minuts

---

## Pas 6: Execuci√≥ de l'Aplicaci√≥ Web

### 6.1. Configurar Mapbox (si cal)

L'aplicaci√≥ utilitza Mapbox GL JS. Si no tens un token de Mapbox configurat, haur√†s d'afegir-lo a les variables d'entorn o modificar el codi del component `WaterMeterMap.tsx`.

### 6.2. Iniciar el Servidor de Desenvolupament

```bash
# Des de l'arrel del projecte
npm run dev
```

L'aplicaci√≥ estar√† disponible a `http://localhost:8080` (o el port que mostri el terminal).

### 6.3. Funcionalitats de l'Aplicaci√≥

- **Mapa interactiu**: Visualitza tots els comptadors amb codi de colors segons el risc
- **Filtres**: Normal (<50%), Warning (50-80%), Alert (‚â•80%)
- **Vista de seccions censals**: Visualitzaci√≥ agregada per √†rees geogr√†fiques
- **Dashboard**: Taula amb tots els comptadors, ordenats per risc
- **Panell d'insights**: Detalls dels 20 comptadors amb major risc
- **Popups al mapa**: Clic sobre un comptador per veure detalls (risc final, subcomptatge, cluster, etc.)

---

## Resum de l'Ordre d'Execuci√≥

```bash
# 1. Preparaci√≥
cd data
# Col¬∑loca el dataset a data/data/Dades_Comptadors_anonymized_v2.parquet
# (o converteix CSV a Parquet)

# 2. Instal¬∑laci√≥
pip install -r requirements.txt
cd ..
npm install

# 3. Creaci√≥ de base de dades
cd data
python create_database.py

# 4. Pipeline ML (en ordre)
python -m stage1_kmeans.run_stage1
python -m stage2_autoencoder.run_stage2
python -m stage3_clustering.run_stage3
python -m stage4_risk_probabilities.run_stage4

# 5. Preparaci√≥ de dades per al mapa
python prepare_map_data.py

# 6. Executar aplicaci√≥ web
cd ..
npm run dev
```

---

## Soluci√≥ de Problemes

### Error: "DuckDB database not found"
- Assegura't d'haver executat `python create_database.py` abans de les etapes.

### Error: "Parquet file not found"
- Verifica que el fitxer estigui a `data/data/Dades_Comptadors_anonymized_v2.parquet`
- Si tens CSV, converteix-lo a Parquet abans.

### Error: "Module not found"
- Assegura't d'haver instal¬∑lat les depend√®ncies: `pip install -r requirements.txt`
- Executa les etapes des de la carpeta `data/`.

### Error: "CUDA out of memory" (PyTorch)
- L'autoencoder s'entrena per defecte a CPU. Si tens GPU i vols utilitzar-la, modifica `run_stage2.py` per especificar el device.

### Els resultats no apareixen al mapa
- Assegura't d'haver executat `prepare_map_data.py` despr√©s de l'Etapa IV
- Verifica que els fitxers GeoJSON estiguin a `public/data/`
- Refresca el navegador despr√©s de regenerar els fitxers

---

## Estructura de Sortides Esperada

Despr√©s d'executar tot el pipeline, haur√†s de tenir:

```
data/
‚îú‚îÄ‚îÄ analytics.duckdb                    # Base de dades creada
‚îú‚îÄ‚îÄ stage1_outputs/
‚îÇ   ‚îî‚îÄ‚îÄ stage1_physical_features_with_clusters.csv
‚îú‚îÄ‚îÄ stage2_outputs/
‚îÇ   ‚îî‚îÄ‚îÄ latent_representations.csv
‚îú‚îÄ‚îÄ stage3_outputs/
‚îÇ   ‚îú‚îÄ‚îÄ cluster_labels.csv
‚îÇ   ‚îú‚îÄ‚îÄ cluster_analysis_*.csv
‚îÇ   ‚îî‚îÄ‚îÄ visualizations/*.png
‚îú‚îÄ‚îÄ stage4_outputs/
‚îÇ   ‚îú‚îÄ‚îÄ meter_failure_risk.csv
‚îÇ   ‚îú‚îÄ‚îÄ risk_summary_by_cluster.csv
‚îÇ   ‚îî‚îÄ‚îÄ visualizations/*.png
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ stage2_autoencoder.pth
‚îÇ   ‚îî‚îÄ‚îÄ stage3_kmeans_clustering.joblib

public/data/
‚îú‚îÄ‚îÄ water_meters.geojson
‚îú‚îÄ‚îÄ census_sections.geojson
‚îî‚îÄ‚îÄ risk_summary.json
```

Amb aquesta estructura, l'aplicaci√≥ web podr√† carregar i visualitzar tots els resultats.

---

